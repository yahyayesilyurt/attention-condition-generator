# ğŸ—ï¸ Architecture Overview

## Complete Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GAT (Heterogeneous Graph)                     â”‚
â”‚                                                                   â”‚
â”‚    User â†â”€â”€interactsâ”€â”€â†’ Movie                                   â”‚
â”‚     â”‚                                                             â”‚
â”‚     â””â”€â”€â”€interactsâ”€â”€â†’ Book                                        â”‚
â”‚                                                                   â”‚
â”‚  Output: User Embeddings (27785, 32)                            â”‚
â”‚          â†“                                                        â”‚
â”‚  Contains information about BOTH movie and book interactions    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AttentionConditioner (This Module)                  â”‚
â”‚                                                                   â”‚
â”‚  Input:  User Embedding (32-dim)                                â”‚
â”‚          Domain Label: "movie" or "book"                         â”‚
â”‚                                                                   â”‚
â”‚  Process:                                                         â”‚
â”‚    Query: User Embedding                                         â”‚
â”‚    Key/Value: Learnable Global Domain Embedding                 â”‚
â”‚               â”œâ”€ Domain "movie": Represents movie domain         â”‚
â”‚               â””â”€ Domain "book": Represents book domain           â”‚
â”‚                                                                   â”‚
â”‚  Multi-Head Cross-Attention:                                     â”‚
â”‚    - 4 attention heads                                           â”‚
â”‚    - Each head learns different aspects of domain conditioning  â”‚
â”‚                                                                   â”‚
â”‚  Output: Domain-Conditioned User Embedding (32-dim)             â”‚
â”‚          â”œâ”€ For "movie": User representation tuned for movies   â”‚
â”‚          â””â”€ For "book": User representation tuned for books     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Points

### 1. User Embeddings (Input)

- **Source**: GAT model on heterogeneous graph
- **Shape**: (27785, 32)
- **Content**: Contains user's interactions with both movies and books
- **Interpretation**: Rich representation of user preferences across domains

### 2. Domain Embeddings (Learnable)

Two global vectors learned during training:

```python
Domain Embedding "movie" (32-dim):
  - Represents characteristics of movie domain
  - Learned to extract movie-relevant features from user embeddings
  - Used when generating movie recommendations

Domain Embedding "book" (32-dim):
  - Represents characteristics of book domain
  - Learned to extract book-relevant features from user embeddings
  - Used when generating book recommendations
```

### 3. Attention Mechanism

```
User Embedding (mixed movie+book info)
         â†“ [Query]

Cross-Attention with Domain Embedding
         â†“ [Key/Value: movie OR book]

Attention learns:
  - Which aspects of user embedding are relevant for this domain
  - How to weight different user preferences for movie vs book

         â†“

Domain-Conditioned Embedding
  - If domain="movie": Emphasizes movie-relevant user preferences
  - If domain="book": Emphasizes book-relevant user preferences
```

### 4. Why This Works

**Problem**: User embeddings contain information about both domains mixed together.

**Solution**: Attention mechanism + domain embeddings act as a "filter":

- Domain embedding "movie" â†’ Extracts movie-relevant aspects
- Domain embedding "book" â†’ Extracts book-relevant aspects

**Result**: Same user â†’ Different representations for different domains

## Data Flow Example

### User 123's Embeddings:

```python
# Original GAT embedding (mixed)
user_123 = [0.5, -0.3, 0.8, 0.1, ...]  # (32-dim)
# Contains both movie AND book interaction patterns

# After conditioning for MOVIE domain
condition_movie = model(user_123, domain="movie")
# â†’ [0.6, -0.2, 0.9, 0.05, ...]  # (32-dim)
# Emphasizes movie preferences

# After conditioning for BOOK domain
condition_book = model(user_123, domain="book")
# â†’ [0.4, -0.4, 0.7, 0.15, ...]  # (32-dim)
# Emphasizes book preferences

# These are DIFFERENT because attention focused on different aspects
```

### Cosine Similarity Analysis:

```python
similarity(user_123, condition_movie) = 0.92  # High: preserves user info
similarity(user_123, condition_book) = 0.89   # High: preserves user info
similarity(condition_movie, condition_book) = 0.75  # Lower: domains differ
```

## Training Process

### Phase 1: Prepare Dataset

```python
For each user embedding (27785 total):
  Create 2 samples:
    1. (user_embedding, domain="movie")
    2. (user_embedding, domain="book")

Total training samples: 27785 Ã— 2 = 55,570
```

### Phase 2: Train AttentionConditioner

```python
For each batch:
  Input: user_embeddings, domain_labels
  Output: conditioned_embeddings

  Loss:
    - Reconstruction: Keep user information
    - Diversity: Make movie â‰  book representations
    - Cosine: Preserve semantic meaning
```

### Phase 3: Generate Condition Vectors

```python
For each user (27785):
  For each domain (movie, book):
    condition[user, domain] = model(user_emb, domain)

Save: (27785, 2, 32) tensor
```

## What The Model Learns

### Domain Embedding "movie":

- Learns to identify movie-specific preference patterns in user embeddings
- Might focus on: genre preferences, director tastes, movie popularity patterns
- Transforms user embedding to emphasize movie-relevant features

### Domain Embedding "book":

- Learns to identify book-specific preference patterns in user embeddings
- Might focus on: author preferences, topic interests, reading complexity
- Transforms user embedding to emphasize book-relevant features

### Attention Heads:

Each of the 4 heads might learn different aspects:

- Head 1: Content similarity (action movies â†” thriller books)
- Head 2: Popularity patterns (mainstream vs niche)
- Head 3: Temporal patterns (new releases vs classics)
- Head 4: Social patterns (trending items)

## Expected Behavior After Training

### Good Signs:

âœ… Diversity loss > 0: Movie and book conditions are different  
âœ… Cosine similarity 0.85-0.95: User information preserved  
âœ… Attention weights non-uniform: Model learned to focus  
âœ… Validation loss decreasing: Model generalizing well

### What To Avoid:

âŒ Diversity loss â†’ 0: Movie and book conditions are identical (model not learning domain differences)  
âŒ Cosine similarity < 0.7: Too much user information lost  
âŒ All attention heads identical: Heads not specializing  
âŒ Overfitting: Val loss increasing while train loss decreasing

## Summary

**Input**: User embeddings with mixed movie+book interactions  
**Process**: Attention-based domain conditioning  
**Output**: Separate conditions optimized for movie vs book generation  
**Usage**: Feed into diffusion model for domain-specific recommendations

The key insight: **Same user, different domain perspectives** â†’ Better domain-specific recommendations!
